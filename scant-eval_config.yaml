# scant-eval.py config 
# Evaluation Config for wealth-alpaca_lora dataset on llama3.2 1B

output_dir:


# Model Arguments
model:
  _component_: torchtune.models.llama3_2.llama3_2_1b

checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: /
  checkpoint_files: [
    model-00001-of-00001.safetensors,
  ]
  output_dir: ${output_dir}
  model_type: LLAMA3_2

# Environment
device: cuda
dtype: bf16

#######################
#In scant-eval.py

# Tokenizer
tokenizer:
  _component_: 
  path: 
  max_seq_len: null

# Dataset
dataset:
  _component_: torchtune.datasets.alpaca_dataset
  source: 'gbharti/wealth-alpaca_lora'
  train_on_input: True
  packed: False
  split: train

#######################